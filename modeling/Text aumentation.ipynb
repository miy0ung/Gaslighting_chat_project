{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7120,"status":"ok","timestamp":1694001757794,"user":{"displayName":"팀피카소","userId":"15162294313622612394"},"user_tz":-540},"id":"j8Fp9IAyxbfT","outputId":"26712095-9115-4e58-dfe1-ade78b46d612"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"musoVvEExWI9"},"outputs":[],"source":["# utils.py\n","import torch\n","import torch.nn as nn\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForMaskedLM,\n","    AdamW,\n","    get_scheduler\n",")\n","def mask_tokens(tokenizer, input_ids:torch.Tensor, mlm_prob:float=0.15, do_rep_random:bool=True):\n","    '''\n","        Copied from huggingface/transformers/data/data_collator - torch.mask_tokens()\n","        Prepare masked tokens inputs/labels for masked language modeling\n","        if do_rep_random is True:\n","            80% MASK, 10% random, 10% original\n","        else:\n","            100% MASK\n","    '''\n","    labels = input_ids.clone()\n","\n","    probability_matrix = torch.full(labels.shape, mlm_prob)\n","    special_tokens_mask = [\n","        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n","    ]\n","    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value = 0.0)\n","    if tokenizer._pad_token is not None:\n","        padding_mask = labels.eq(tokenizer.pad_token_id)\n","        probability_matrix.masked_fill_(padding_mask, value=0.0)\n","    masked_indices = torch.bernoulli(probability_matrix).bool()\n","    labels[~masked_indices] = -100 # We only compute loss on masked tokens\n","\n","    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n","    mask_rep_prob = 0.8\n","    if not do_rep_random:\n","        mask_rep_prob = 1.0\n","\n","    indices_replaced = torch.bernoulli(torch.full(labels.shape, mask_rep_prob)).bool() & masked_indices\n","    input_ids[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n","\n","    if do_rep_random:\n","        # 10% of the time, we replace masked input tokens with random word\n","        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n","        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n","        input_ids[indices_random] = random_words[indices_random]\n","\n","    return input_ids, labels\n","\n","\n","def load_tokenizer(args):\n","    if args.do_pred:\n","        tokenizer_path = args.tuned_model_path\n","    else:\n","        tokenizer_path = args.model_name_or_path\n","\n","    return AutoTokenizer.from_pretrained(tokenizer_path)\n","\n","def initialize_model(args, total_steps):\n","    model = AutoModelForMaskedLM.from_pretrained(args.model_name_or_path)\n","\n","    if (torch.cuda.is_available()) and (not args.no_cuda):\n","        if (not args.multi):\n","            device = \"cuda:\" + str(args.dev_num)\n","        else:\n","            n_dev = torch.cuda.device_count()\n","            dev_list = list(range(n_dev))\n","            model = nn.DataParallel(model, device_ids = dev_list, output_device=dev_list[0])\n","            device = dev_list[0]\n","    else:\n","        device = \"cpu\"\n","    model.to(device)\n","\n","    optimizer = AdamW(model.parameters(),\n","                    lr = args.learning_Rate,\n","                    eps = args.eps,\n","                    weight_decay = args.weight_decay)\n","\n","    scheduler = get_scheduler(args.scheduler_name,\n","                            optimizer,\n","                            num_warmup_steps = int(total_steps * args.warmup_proportion),\n","                            num_training_steps = total_steps)\n","\n","    return model, optimizer, scheduler, device\n","\n","\n","def initialize_model_with_ds(args):\n","    import deepspeed\n","\n","    model = AutoModelForMaskedLM.from_pretrained(args.model_name_or_path)\n","    model, optimizer, _, scheduler = deepspeed.initialize(model=model, args=args, model_parameters=model.parameters())\n","\n","    return model, optimizer, scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBAEnBAYxqVJ"},"outputs":[],"source":["# data_loader.py\n","import torch\n","import pandas as pd\n","\n","from tqdm import tqdm\n","from torch.utils.data import Dataset\n","from transformers import AutoTokenizer\n","from argparse import Namespace\n","\n","class DataSet(Dataset):\n","    def __init__(self, df:pd.DataFrame, tokenizer:AutoTokenizer, args:Namespace):\n","        self.data = df.to_dict(\"records\")\n","        input_ids, attention_masks = [], []\n","\n","        for line in tqdm(self.data):\n","            try:\n","                comments = line[\"comments\"].replace(\"\\n\", \"\")\n","                encoded_dict = tokenizer(\n","                    comments,\n","                    add_special_tokens =True,\n","                    max_length = args.max_seq_len,\n","                    padding = \"max_length\",\n","                    truncation = True,\n","                    return_attention_mask = True,\n","                    return_tensors=\"pt\"\n","                )\n","                input_ids.append(encoded_dict.input_ids)\n","                attention_masks.append(encoded_dict.attention_mask)\n","            except:\n","                continue\n","\n","        # flattening : convert it to 0 dim torch tensor\n","        self.input_ids = torch.cat(input_ids, dim = 0)\n","        self.attention_masks = torch.cat(attention_masks, dim = 0)\n","\n","\n","    # get data length\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    # get each data info\n","    def __getitem__(self, idx):\n","        input_id = self.input_ids[idx]\n","        attention_mask = self.attention_masks[idx]\n","\n","        return input_id, attention_mask\n","\n","\n","\n","class AugmentDataSet(Dataset):\n","    def __init__(self, sent_list:list, tokenizer:AutoTokenizer, args:Namespace):\n","        total_result = []\n","        input_ids, attention_masks = [], []\n","\n","        for line in tqdm(sent_list):\n","            try:\n","                comments = line\n","                encoded_dict = tokenizer(\n","                    comments,\n","                    add_special_tokens =True,\n","                    max_length = 100,\n","                    padding = \"max_length\",\n","                    truncation = True,\n","                    return_attention_mask = True,\n","                    return_tensors=\"pt\"\n","                )\n","                input_ids.append(encoded_dict.input_ids)\n","                attention_masks.append(encoded_dict.attention_mask)\n","                total_result.append({\"comments\":line})\n","            except:\n","                continue\n","\n","        # flattening : convert it to 0 dim torch tensor\n","        self.input_ids = torch.cat(input_ids, dim = 0)\n","        self.attention_masks = torch.cat(attention_masks, dim = 0)\n","        self.df = pd.DataFrame(total_result)\n","\n","    # get data length\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    # get each data info\n","    def __getitem__(self, idx):\n","        input_id = self.input_ids[idx]\n","        attention_mask = self.attention_masks[idx]\n","\n","        return input_id, attention_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8297,"status":"ok","timestamp":1694001766084,"user":{"displayName":"팀피카소","userId":"15162294313622612394"},"user_tz":-540},"id":"xe3VGIOixcfs","outputId":"0a9237e7-8f9b-4e7e-c3ad-8e4351431a28"},"outputs":[{"output_type":"stream","name":"stdout","text":["INPUT = quit\n","대화가 종료됩니다.\n"]}],"source":["# augment.py\n","import copy\n","import torch\n","import tqdm\n","import argparse\n","\n","#from utils import mask_tokens\n","\n","from typing import Union\n","from torch.utils.data import DataLoader\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","\n","def load_tuned_model(args:argparse.Namespace):\n","    if (torch.cuda.is_available()) and (args.dev_num>=0) and (args.dev_num < torch.cuda.device_count()):\n","        dev = \"cuda:{}\".format(args.dev_num)\n","    else:\n","        dev = \"cpu\"\n","\n","    model = AutoModelForMaskedLM.from_pretrained(args.tuned_model_path)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tuned_model_path)\n","\n","    model.to(dev)\n","    return model, tokenizer, dev\n","\n","def tokenize(tokenizer:AutoTokenizer, sent:str):\n","    encoded_dict = tokenizer(\n","        sent,\n","        add_special_tokens = True,\n","        return_attention_mask = True,\n","        return_tensors = \"pt\"\n","    )\n","    input_id, attention_mask = encoded_dict.input_ids, encoded_dict.attention_mask\n","\n","    return input_id, attention_mask\n","\n","def is_same_token_type(org_token:str, candidate:str) -> bool:\n","    '''\n","    후보 필터링 조건을 만족하는지 확인\n","    - 후보와 원 토큰의 타입을 문장부호와 일반 토큰으로 나누어 같은 타입에 속하는지 확인\n","    '''\n","    res = False\n","    if org_token[0]==\"#\" and org_token[2:].isalpha()==candidate.isalpha():\n","        res = True\n","    elif candidate[0]==\"#\" and org_token.isalpha()==candidate[2:].isalpha():\n","        res = True\n","    elif candidate[0]==\"#\" and org_token[0]==\"#\" and org_token[2:].isalpha()==candidate[2:].isalpha():\n","        res = True\n","    elif org_token.isalpha()==candidate.isalpha() and (candidate[0]!=\"#\" and org_token[0]!=\"#\"):\n","        res = True\n","\n","    return res\n","\n","def candidate_filtering(tokenizer:AutoTokenizer,\n","                        input_ids:list,\n","                        idx:int,\n","                        org:int,\n","                        candidates:Union[list, torch.Tensor]) -> int:\n","    '''\n","    후보 필터링 조건에 만족하는 최적의 후보 선택\n","    1. 원래 토큰과 후보 토큰이 같은 타입(is_same_token_type 참고)\n","    2. 현 위치 앞 혹은 뒤에 동일한 토큰이 있지 않음\n","    '''\n","\n","    org_token = tokenizer.convert_ids_to_tokens([org])[0]\n","    candidate_tokens = tokenizer.convert_ids_to_tokens(candidates.cpu().tolist())\n","\n","    for rank, token in enumerate(candidate_tokens):\n","        if org_token!=token and is_same_token_type(org_token, token):\n","            if input_ids[idx-1]==candidates[rank] or input_ids[idx+1]==candidate_tokens[rank]:\n","                continue\n","            return candidates[rank]\n","\n","    return org\n","\n","def augment_one_sent(model:AutoModelForMaskedLM,\n","                    tokenizer:AutoTokenizer,\n","                    sent:str,\n","                    dev:Union[str, torch.device],\n","                    args:Union[argparse.Namespace, dict]) -> str:\n","    '''\n","    한 문장에 랜덤으로 마스킹을 적용하여 새로운 문장을 생성(증강)\n","\n","    args:\n","        model(AutoModelForMaskedLM)     : finetuned model\n","        tokenizer(AutoTokenizer)\n","        sent(str)                       : 증강할 문장\n","        dev(str or torch.device)\n","        args(argparse.Namespace)\n","            - k(int, default=5) : 사용할 후보의 개수. k개의 후보 적절한 토큰이 없을 경우 원래 토큰 그대로 유지\n","            - threshold(float, default=0.95) : 확률 필터링에 사용할 임계치.\n","                                               마스크에 대해서 특정 후보 토큰을 생성할 확률이 임계치보다 클 경우에는 별도의 필터링 없이 후보를 그대로 사용.\n","           -  mlm_prob(float, default=0.15) : 마스킹 비율\n","\n","    return:\n","        (str) : 증강 문장\n","    '''\n","\n","    if type(args) == argparse.Namespace:\n","        k = args.k\n","        threshold = args.threshold\n","        mlm_prob = args.mlm_prob\n","    else:\n","        ## type == dict\n","        k = args[\"k\"]\n","        threshold = args[\"threshold\"]\n","        mlm_prob = args[\"mlm_prob\"]\n","\n","    model.eval()\n","\n","    input_id, attention_mask  = tokenize(tokenizer, sent)\n","    org_ids = copy.deepcopy(input_id[0])\n","\n","    masked_input_id, _ = mask_tokens(tokenizer, input_id, mlm_prob, do_rep_random=False)\n","    while masked_input_id.cpu().tolist()[0].count(tokenizer.mask_token_id) < 1:\n","        masked_input_id, _ = mask_tokens(tokenizer, input_id, mlm_prob, do_rep_random=False)\n","\n","    with torch.no_grad():\n","        masked_input_id, attention_mask = masked_input_id.to(dev), attention_mask.to(dev)\n","        output = model(masked_input_id, attention_mask = attention_mask)\n","        logits = output[\"logits\"][0]\n","\n","    copied = copy.deepcopy(masked_input_id.cpu().tolist()[0])\n","    for i in range(len(copied)):\n","        if copied[i] == tokenizer.mask_token_id:\n","            org_token = org_ids[i]\n","            prob = logits[i].softmax(dim=0)\n","            probability, candidates = prob.topk(k)\n","            if probability[0]<threshold:\n","                res = candidate_filtering(tokenizer, copied, i, org_token, candidates)\n","            else:\n","                res = candidates[0]\n","            copied[i] = res\n","\n","    copied = tokenizer.decode(copied, skip_special_tokens=True)\n","\n","    return copied\n","\n","\n","def batch_augment(model:AutoModelForMaskedLM,\n","                tokenizer:AutoTokenizer,\n","                dataset:torch.utils.data.Dataset,\n","                dev:Union[str, torch.device],\n","                args:argparse.Namespace) -> str:\n","    '''\n","    배치 단위의 문장에 랜덤으로 마스킹을 적용하여 새로운 문장 배치를 생성(증강)\n","\n","    args:\n","        model(AutoModelForMaskedLM)\n","        tokenizer(AutoTokenizer)\n","        dataset(torch.utils.data.Dataset)\n","        dev(str or torch.device)\n","        args(argparse.Namespace)\n","            - k(int, default=5)\n","            - threshold(float, default=0.95)\n","           -  mlm_prob(float, default=0.15)\n","\n","    return:\n","        (list) : 증강한 문장들의 리스트\n","    '''\n","\n","    k = args.k\n","    threshold = args.threshold\n","    mlm_prob = args.mlm_prob\n","    batch_size = args.batch_size\n","\n","    model.eval()\n","\n","    augmented_res = []\n","    dataloader = DataLoader(dataset, batch_size = batch_size)\n","    for batch in tqdm.tqdm(dataloader):\n","        #########################################################\n","        # 인풋 문장에 랜덤으로 마스킹 적용\n","        input_ids, attention_masks = batch[0], batch[1]\n","        masked_input_ids, _ = mask_tokens(tokenizer, input_ids, mlm_prob, do_rep_random=False)\n","\n","        masked_input_ids = masked_input_ids.to(dev)\n","        attention_masks = attention_masks.to(dev)\n","        labels = input_ids\n","        #########################################################\n","\n","        with torch.no_grad():\n","            output = model(masked_input_ids, attention_mask = attention_masks)\n","            logits1 = output[\"logits\"]\n","\n","        #########################################################\n","        # 배치 내의 문장 별로 후보 필터링을 적용하고, 결과를 토대로 새로운 문장 생성\n","        augmented1 = []\n","        for sent_no in range(len(masked_input_ids)):\n","            copied = copy.deepcopy(input_ids.cpu().tolist()[sent_no])\n","\n","            for i in range(len(masked_input_ids[sent_no])):\n","                if masked_input_ids[sent_no][i] == tokenizer.pad_token_id:\n","                    break\n","\n","                if masked_input_ids[sent_no][i] == tokenizer.mask_token_id:\n","                    org_token = labels.cpu().tolist()[sent_no][i]\n","                    prob = logits1[sent_no][i].softmax(dim=0)\n","                    probability, candidates = prob.topk(k)\n","                    if probability[0]<threshold:\n","                        res = candidate_filtering(tokenizer, copied, i, org_token, candidates)\n","                    else:\n","                        res = candidates[0]\n","                    copied[i] = res\n","\n","            copied = tokenizer.decode(copied, skip_special_tokens=True)\n","            augmented1.append(copied)\n","        #########################################################\n","        augmented_res.extend(augmented1)\n","\n","    return augmented_res\n","\n","if __name__ == \"__main__\":\n","    import random\n","\n","    random.seed(1)\n","\n","    args = argparse.Namespace(\n","        tuned_model_path=\"seoyeon96/KcELECTRA-MLM\",\n","        dev_num=0,\n","        input_file=None,\n","        batch_size=1,\n","        mlm_prob=0.15,\n","        threshold=0.95,\n","        k=5\n","    )\n","\n","    model, tokenizer, dev = load_tuned_model(args)\n","\n","    if args.batch_size > 1:\n","        if args.input_file is None:\n","            raise Exception(\"input_file is None\")\n","\n","        with open(args.input_file, \"r\") as f:\n","            corpus = f.readlines()\n","\n","        dataset = AugmentDataSet(corpus, tokenizer)\n","        augmented = batch_augment(model, tokenizer, dataset, dev, args)\n","    else:\n","        while True:\n","            input_sen = input(\"INPUT = \").strip()\n","            if input_sen.lower() == \"quit\":\n","                print(\"대화가 종료됩니다.\")\n","                break\n","            augmented = augment_one_sent(model, tokenizer, input_sen, dev, args)\n","            print(\"OUTPUT = \", augmented)\n","            print(\"-\"*30)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2480,"status":"ok","timestamp":1694001768560,"user":{"displayName":"팀피카소","userId":"15162294313622612394"},"user_tz":-540},"id":"oiHS308p3xJI","outputId":"6da4e12b-fcf2-415c-ec1b-37dadda9e5a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgCOtetu8Oy2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c3df2b70-1ff0-4f6c-c198-828a10be9382"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                 prompt  \\\n","0     오빠가 그런 행동 한다는 것 자체가 나한테는 계속 상처인데 평생 사죄를 하고 케어하...   \n","1           그런 행동들이 나한테는 상처라고 그런데 어떻게 케어를 하고 케어를 하겠다는거야   \n","2      누가 그런 행동을 했다는거야 난 너무 상처받았는데 어떻게 위로를 하고 위로를 한다는거야   \n","3                     그딴 행동을 하지 말았어야지 이제와서 뭘 하겠다는거야 도대체   \n","4     계속 그런식으로 행동하면서 상처받았으면서 부모님에게 사죄도 하고 케어하겠다는 말은 ...   \n","...                                                 ...   \n","3393                                    오늘 안주에 귀신이 나왔어요   \n","3394                       넌 세상이 얼마나 만만하게 보이면 그런행동을 하냐?   \n","3395                           담배 좀 끊어라 무슨 죽을라고 그렇게 피냐?   \n","3396             아이가 수업시간에 돌아다녀 다른 아이들이 집중을 하고 있습니다 어머니   \n","3397                           난 그런 기사 좀 불편하니까 안썻으면 좋겠어   \n","\n","                                           competition  \n","0                        자꾸 고집 피우지마 난 그런 말 한 적이 없는데거든?  \n","1                       아니 언제 저런 행동을 했다는거야 고집 부리 내지좀 마  \n","2                      그런 행동이 뭔데? 내가 무슨 행동을 했다는거야 왜 우겨  \n","3                너 지금 나한테 그딴 행동이라고 한거야. 그리고 너 니가 뭘 알아어  \n","4                             내가 또 왜 보라고 그래? 이번이 처음이잖아  \n","...                                                ...  \n","3393     머리카락이 일부러 넣은건 아니시고요? 제 머리카락이 나올리가 없는데 그냥 드시지요  \n","3394                   싫으면 말지만 왜 그렇게 해서 남한테 상처를 주냐 너는?  \n","3395                누가 좋아서 피는 줄 알아? 배운 게 이거밖에 안되니까 그런다  \n","3396  애가 그럴수도 있지 뭘 그래요 선생님이 수업 얼마나 잘 하셨으면 우리 애가 그랬을까요?  \n","3397                너 그렇게 모든일에 안일하게 굴지면 보는 사람도 다 떠나갈걸?  \n","\n","[3398 rows x 2 columns]\n"]}],"source":["import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","import copy\n","import torch\n","\n","\n","# 모델 및 토크나이저 로딩\n","args = argparse.Namespace(\n","    tuned_model_path=\"seoyeon96/KcELECTRA-MLM\",\n","    dev_num=0,\n","    input_file=None,\n","    batch_size=1,\n","    mlm_prob=0.15,\n","    threshold=0.95,\n","    k=5\n",")\n","\n","model, tokenizer, dev = load_tuned_model(args)\n","\n","# 데이터 로딩\n","df = pd.read_excel(\"/content/drive/MyDrive/NLPicasso/최종/Gaslighting_data.xlsx\")\n","\n","# \"prompt\" 및 \"competition\" 열의 텍스트 증강\n","augmented_data = []\n","for _, row in df.iterrows():\n","    for column in [\"prompt\", \"competition\"]:\n","        # Convert the data to string type before passing\n","        text_data = str(row[column])\n","        augmented_text = augment_one_sent(model, tokenizer, text_data, dev, args)\n","        row[column] = augmented_text\n","    augmented_data.append(row)\n","\n","\n","\n","# 증강된 데이터와 원본 데이터 합치기\n","augmented_df = pd.DataFrame(augmented_data)\n","combined_df = pd.concat([df, augmented_df], ignore_index=True)\n","combined_df = combined_df.drop(['Unnamed: 2'], axis = 1)\n","\n","\n","print(combined_df)\n","\n","# 엑셀 파일로 저장\n","combined_df.to_csv(\"/content/drive/MyDrive/NLPicasso/최종/Gaslighting_augmentation_data.csv\", index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}